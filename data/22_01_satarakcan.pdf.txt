Language Learning & Technology 
ISSN 1094-3501 
February 2018, Volume 22, Issue 1 
pp. 157–183 
ARTICLE  
 
 
Copyright © 2018 H. Müge Satar & Sumru Akcan 
 
 
Pre-service EFL teachers’ online participation, 
interaction, and social presence 
H. Müge Satar, Newcastle University 
Sumru Akcan, Boğaziçi University 
Abstract 
Participation in online communities is an increasing need for future language teachers and their 
professional development. Through such participation, they can experience and develop an awareness of 
the behaviors required to facilitate their future learners’ participation in online learning. This article 
investigates participation, interaction patterns, and social presence (SP) levels of pre-service English as 
a foreign language (EFL) teachers in online communication within a longitudinal blended learning 
setting. A secondary aim of this article is to explore social network analysis (SNA) as an alternative 
method to measure SP. Data analysis included calculation of number of forum entries and words, 
qualitative analysis of interaction patterns, content analysis, and SNA. The results indicated that an 
online course on tutoring skills and SP improved pre-service EFL teachers’ online participation skills. 
Increased interaction and a more cohesive network were observed as the course progressed. The findings 
are significant in that they suggest a relationship between content analysis for SP (especially the 
interactive dimension) and SNA measures (centrality, influence, and prestige), implicating SNA as an 
emerging research method for the investigation of SP. This article concludes with future research 
perspectives and suggestions for EFL teacher training. 
Keywords: Blended Learning and Teaching, Teacher Education, Research Methods, Computer-Mediated 
Communication 
Language(s) Learned in this Study: English 
APA Citation: Satar, H. M., & Akcan, S. (2018). Pre-service EFL teachers’ online participation, 
interaction, and social presence. Language Learning & Technology, 22(1), 157–183. 
https//dx.doi.org/10125/44586 
Introduction 
With increasing availability of the Internet and computer-mediated communication (CMC) contexts, 
language learners and teachers have been observed to improve their knowledge of foreign language and 
language pedagogy by sharing, reflection, evaluation, collaboration, and problem-solving activities 
(Arnold & Ducate, 2006; Dooly & O’Dowd, 2012; Meskill, 2009; Richardson, 2006). In foreign language 
teacher education, pre-service teachers need to acquire not only the skills necessary to actively and 
productively participate in online interaction, but also the skills required to facilitate their students’ 
participation and interaction in online contexts. One way to equip pre-service language teachers with 
these skills is to involve them in blended learning contexts that can offer them opportunities to develop 
these skills through experiential modeling (Hoven, 2006) and reflective practice (Farrell, 2007; Freese, 
2006; Loughran, 2002). However, ensuring continued participation and interaction in online platforms is a 
challenge (Yuan & Kim, 2014; Whiteside, 2015). Besides, participation and interaction do not necessarily 
lead to collaboration unless an optimal level of social presence (SP) is achieved (Zhao, Sullivan, & 
Melleius, 2014). Moreover, although SP levels of online participants have been observed to vary over 
time (e.g., Shea et al., 2010; Swan & Shih, 2005), the findings of these studies have been inconclusive. 
Finally, studies that employ social network analysis (SNA) and content analysis for the investigation of 
158 Language Learning & Technology 
 
SP are rare (Shea et al., 2010; Wu, Gao, & Zhang, 2014) and their results are contradictory. 
Therefore, the present study explores the implementation of blended learning in two English as a foreign 
language (EFL) teacher education practicum courses delivered over two semesters. Pre-service teachers’ 
participation and interaction patterns, as well as their SP levels, are investigated to observe the 
development of their participation and interaction skills. By examining the relationship between the 
centrality of pre-service teachers in the online community and their SP levels, this article proposes SNA 
as an emerging alternative method to study the concept of SP. 
Literature Review 
This section defines the concepts of participation and interaction in blended learning settings, and 
explains the theory of social presence and its components. Next, SNA is introduced, and the section 
concludes with the ways in which SNA is used to investigate participation, interaction, and social 
presence in personal and educational online settings. 
Participation and Interaction in Blended Learning 
Blended learning contexts can potentially support face-to-face teaching by web-based teaching (Garrison 
& Kanuka, 2004), increase student–student and student–teacher interaction opportunities (Michinov & 
Michinov, 2008), and enhance learning (Lim & Morris, 2009; O’Toole & Absalom, 2003). However, 
smooth management of blended learning programs is challenging for teachers in that they require 
instructors “to think critically about the affordances of the different media in order to continually engage 
students in meaningful learning and to maintain social presence” (Whiteside, 2015, p. 16). Dropout 
problems and insufficient amount of participation in professional online communities or online teaching 
programs have also been documented as major challenges (Yuan & Kim, 2014). 
In blended learning contexts, instructors’ roles as well as socio-affective bonding among participants 
seem to be influential in the amount of participation and interaction. Exploring experiences of engineering 
undergraduate learners and instructors qualitatively, Szeto (2015) observed that the instructor’s role as a 
leader was the most salient factor influencing the design and implementation of the synchronous 
(videoconferencing) blended program and the attainment of intended learning outcomes. Pawan, Paulus, 
Yalcin, and Chang (2003) explored learner participation in online asynchronous discussions for three 
online postgraduate language teacher education courses by quantifying total number of posts, average 
number of posts per student, and range of posts per student. They identified an uneven amount of 
participation among students and low instructor participation. Moreover, Çelik (2013) explored class 
dynamics and socio-affective relationships among pre-service EFL teachers in Turkey in a discussion 
board as part of a blended course and concluded that negative attitudes were responsible for the limited 
participation of inexperienced learners. 
Nevertheless, low level of participation is not always the case. For instance, investigating online 
participation patterns of in-service teachers in Singapore for a course on integrating technology into the 
classroom, Sing and Khine (2006) reported high levels of task-focused interaction and participation in 
terms of the messages read and comments written in response to others’ messages. Hara, Bonk, and 
Angeli (2000) analyzed participation patterns of college students in an online discussion and found 
increasingly continuous and engaging discussions over time where learners contributed to multiple 
message threads simultaneously. Zheng and Warschauer (2015) measured student participation in online 
asynchronous discussions throughout a school year by quantifying number of posts by students and 
observed increased online participation over time with highest levels of participation in month seven of an 
online discussion lasting eight months. They concluded that online interactions became more dynamic 
and dense with decreasing teacher dominance. Comparing participation levels with results from SNA, 
they showed that online interactions were most beneficial for increasing participation levels of lower-
performing students who were initially at the periphery of the community. Investigating time as a variable 
in online interaction patterns of teachers, Wu et al. (2014) found that participation increased over time. 
H. Müge Satar and Sumru Akcan 159 
 
However, they concluded that time “did not contribute to marked and incremental development of socio-
cognitive presence” (p. 248). 
Yet, according to Zhao et al. (2014), participation and interaction do not necessarily directly lead to 
collaboration in asynchronous discussion boards. They explored interactions of six peer review groups of 
18 tertiary students and argued that collaboration required not only participation, evidenced by posting of 
messages, but also interactive and warm participation, which could be achieved by “an optimal level of 
social presence” (p. 817). Accordingly, they describe participation as independent messages posted on the 
discussion without any peer response; interaction as one-way interaction where an independent message 
receives peer-feedback, but is not followed by a response from the writer of the first message or from 
another peer; and collaboration as two- and multiple-way interaction where a writer receives peer 
feedback that is followed by further comments from the writer or other peers. These studies indicate the 
need for further research on the development of participation and interaction patterns of pre-service 
language teachers in online communities as part of blended courses in order to better understand the 
extent to which such courses can support advancement of language teacher education. 
Social Presence 
SP has been established as a crucial construct for learning online. It has been found to positively predict 
perceived learner satisfaction in CMC (Gunawardena & Zittle, 1997; Tsai, 2012) and to create a sense of 
belonging, group commitment, and community feeling among online learners (Arnold, Ducate, Lomicka, 
& Lord, 2005; Remesal & Colomina, 2013; Tu & McIsaac, 2002; Wegerif, 1998). SP has also been found 
to promote participation, collaboration (Zhao et al., 2014), and group cohesion (Murphy, 2004). 
As a component of the community of inquiry (CoI) framework (Garrison, Anderson, & Archer, 1999), the 
concept of SP is defined as “the ability of learners to project themselves socially and affectively into a 
community of inquiry” (Rourke, Anderson, Garrison, & Archer, 1999, p. 50). Rourke et al. (1999) 
established a content analysis framework to investigate SP in asynchronous online discussions. The 
framework consisted of affective, interactive, and cohesive indicators. These indicators were used to 
investigate SP in several online learning contexts and were later adapted by many researchers to suit the 
particular research context (e.g., Lowenthal, 2012; King & Ellis, 2009; Satar, 2007, 2015; Shea et al., 
2010). Yet despite minor adaptations, the indicators have essentially remained unchanged (Lowenthal & 
Dunlap, 2014). 
Within a CoI, where cognitive and teaching presence are the other components, SP is considered a 
mediating variable between teaching and cognitive presence, maintaining a conducive environment for 
cognitive development (Garrison, Cleveland-Innes, & Fung, 2010). Based on the sociocultural learning 
theory of Vygotsky (1978), which suggests that knowledge is co-constructed in social interaction, SP 
“provides a necessary level of facilitation to support students as they collaboratively construct knowledge 
and self- or co-regulate their learning” (Shea et al., 2014, p. 15). 
In contrast to the view of SP in CoI as a mediating variable, Hauck and Warnecke (2013) indicated that 
SP is not only a facilitator of cognitive presence, as suggested by Garrison et al. (1999); on the contrary, 
SP is a basic e-literacy skill that involves the ability to send, read, and interpret SP indicators. Hauck and 
Warnecke (2013) support Kehrwald’s (2010) proposition that SP is developed in interaction through 
experiential learning, that is, “through seeing and experiencing ... how others interact with one another 
and how others react to their personal efforts to cultivate a social presence” (p. 47). 
Research on SP, as observed in online interactions, indicates that SP seems to vary over time. Vaughan 
and Garrison (2006) explored SP among a faculty learning community of educators in blended learning 
with face-to-face and asynchronous online discussions. They identified a decline in affective and 
interactive measures and a dramatic increase in cohesive measures over time in the online setting, while 
no changes were detected in the face-to-face context. However, in asynchronous online discussions for 
graduate courses in educational technology, Swan and Shih (2005) observed a decrease in cohesive 
indicators over time. According to Garrison and Arbaugh (2007), there should be an overall decrease in 
160 Language Learning & Technology 
 
SP measures over time, as the focus of interactions should shift to academic matters. However, Shea et al. 
(2010, p. 12) compared two online courses in business management and found a decrease in all three SP 
indicators per student over time only in one of these courses. In the other course, there was an overall 
increase in the amount of SP with a decrease only in affective indicators. Yet, neither of these studies 
explored variance in pre-service teachers’ projection of various SP indicators over time in asynchronous 
online discussions. The present study aims to fill this gap in the literature. 
Social Network Analysis 
One research tool to investigate interaction in online communication is SNA, a visual and statistical 
analysis that aims to explore the relationships among individuals in a social network (Prell, 2012; Scott, 
2013; Wasserman & Faust, 1994). SNA allows researchers to investigate networks at either group 
(sociocentric approach) or actor (i.e., individual) level (egocentric approach). At group level analysis, it is 
possible to calculate a density score to understand the overall level of connectedness in a group. A higher 
density score indicates a more connected, cohesive group. At actor level, a centrality score can be 
calculated using SNA. Centrality indicates the amount of interaction between an individual and other 
members of a network. In online discussions, while overall network centrality of an individual includes 
both mutual and unreciprocated relations or messages, indegree centrality is a measure of the number of 
messages individuals receive in response to an earlier message of theirs and outdegree centrality is a 
measure of the number of messages an individual sends in response to the messages of other participants. 
As indegree centrality measures incoming responses, it is considered an indicator of popularity or 
prestige. As outdegree centrality measures outgoing responses, it is considered an indicator of influence, 
involvement, activity in a network (Prell, 2012). Using SNA, the connection among individuals in a social 
network can be represented visually in a social network diagram, called a sociogram. 
The Use of SNA to Investigate Participation, Interaction, and SP 
Several studies in personal and educational contexts have used SNA to investigate participation and 
interaction in online communities. Baek and Kim (2015) used SNA to examine interaction patterns of 
participants in two Korean informal online discussion communities: a personal interest focused 
community and a social interest focused community. Their findings indicated that the nature of discussion 
topics affected participants’ interaction patterns. In formal educational contexts, however, different 
motivations for participation and the teachers’ role need to be considered. Aviv, Erlich, Ravid, and Geva 
(2003) used SNA to investigate knowledge construction processes and group cohesion in two 
asynchronous learning networks as part of a business ethics course. In structured networks, they observed 
higher knowledge construction processes and smaller cohesive groups without a central role for the 
teacher. Shea et al. (2010) also focused on the teacher’s role in an online business management course. 
They concluded that typical measures of centrality “appear to be relatively poor indicators of productive 
interaction, especially when applied to what might be considered a very central participant, the instructor” 
(p. 17). According to Shea et al., network prestige (indegree centrality) could be a more appropriate 
indicator of the interactions of a strategic instructor, who posts fewer comments that are directed at 
course requirements and that can elicit a high number of responses. 
Shea et al. (2010) observed similar patterns between density measures of SNA and SP measures obtained 
through content analysis and proposed that SNA might be an alternative method to labor intensive content 
analysis. In a subsequent study, Shea et al. (2014) explored learner interactions in an online doctoral level 
research methods course for the relationships between the components of the CoI framework (social, 
cognitive, learner, and teacher presences) and SNA measures of centrality, prestige, and influence. Their 
findings suggested that learners with high levels of social, cognitive, and learner presences were central to 
the network. The only exception was teacher presence indicators, which did not always correlate with 
centrality measures. 
SNA has been used in some language learning and teaching contexts. For instance, Reffay and Chanier 
(2003) investigated cohesion in discussion forums in a French language course. Duensing, Stickler, 
H. Müge Satar and Sumru Akcan 161 
 
Batstone, and Heins (2006) explored interaction patterns in online synchronous audio communication in a 
German course. Yet, research that investigates online interaction in language learning or language teacher 
education through SNA and content analysis of SP indicators is rare. One such example is by Wu et al. 
(2014), who tracked interaction patterns of three Chinese EFL teachers in professional networks over 
eight years. They employed both SNA and content analysis to measure the amount of social and cognitive 
presence. Their results seemed to contradict the studies of Shea et al. (2010, 2014) as Wu et al. (2014) 
suggested that “peripheral members who had a strong desire to communicate with the key figures of the 
community” (p. 247) used a higher number of SP indicators. They observed teachers initially attempting 
to form vertical relationships, contacting more powerful actors in their network. But in time, teachers 
increasingly interacted with novice participants as well as experts. It is perhaps important to note here that 
the educational context of the studies by Shea et al. (2010, 2014) was different from the context of the 
study by Wu, et al. (2014) where language teachers participated in professional networks. 
Therefore, although the role of SP in promoting a collaborative and cohesive online learning community 
seems to be widely accepted, the results of earlier studies on the relationship between the outcomes of two 
major methods used to investigate participation and interaction (i.e., content analysis for SP and centrality 
measures of SNA) are inconclusive. This article aims to contribute to the knowledge base (a) by 
investigating participation patterns, interaction patterns, and SP levels of learners using content analysis 
and SNA and (b) by exploring the relationships between the outcomes of SP and SNA measures to test 
whether SNA can be an alternative method to content analysis to measure SP. 
Methods 
Data for this article were collected and analyzed within a case study methodology (Creswell, 2007; 
Gillham, 2000; Yin, 2003), because within a case study design, it was possible to collect multiple sources 
of data using multiple methods of analysis. For the investigation of participation patterns, interaction 
patterns and SP, mainly quantitative data were collected and analyzed. Qualitative data analysis was 
conducted to explain, validate, and further investigate the results of quantitative analysis. Case study 
methodology also allowed us to employ both content analysis and SNA to better understand the 
relationship between SP and network centrality. The bounded entity studied here was composed of two 
blended learning pre-service language teacher education practicum courses, and, in particular, their online 
components. 
Context and Participants 
Data for this study were collected in the 2012–2013 academic year as part of two undergraduate fourth-
year language teacher education courses at the foreign language education department of a university in 
Turkey. Further information on the program can be found in the article by Satar and Akcan (2014). The 
courses were taught in fall and spring semesters with face-to-face and online class components, as well as 
opportunities for teaching experience at practicum schools (primary, secondary, and high schools). At 
their practicum schools, the students predominantly conducted observations during the fall semester and 
taught six observed classes during the spring semester. In-class and online discussions comprised the 
theoretical aspect of the courses and acted as a venue for reflective practice. During face-to-face 
components of the courses, a specific element of the foreign language classroom was introduced and 
discussed each week. Pre-service teachers then observed those elements at their practicum schools and 
reflected on their observations. Each semester lasted 14 weeks. 
For the online component, a free online tool, Canvas by Instructure, was used as the learning management 
system hosting the tasks, asynchronous discussion posts, and introduction pages for each participant. The 
online component of the fall semester was an online tutoring skills training developed by Hauck and 
Warnecke (2013). The course aimed at developing pre-service teachers’ awareness of online participation 
skills and SP. While participants discussed certain issues of classroom teaching in the face-to-face setting, 
they were provided with opportunities to discuss similar issues for online teaching in the online 
162 Language Learning & Technology 
 
component, such as ice-breaker activities and participation patterns. Having completed the fall term 
online tutoring skills training, participants were invited to take part in an online component for the spring 
term course, which served as a platform for extended discussions of their face-to-face teaching 
experiences. Pre-service teachers also shared teaching materials developed using several online tools. 
Moreover, online components provided pre-service teachers with a first-hand experience of and reflection 
on online learning and participation in a blended learning community. Participation in the online 
component comprised 10% of the overall course grades. 
Online tasks were completed outside of class and each task lasted about two weeks. Fall semester online 
discussions continued for nine weeks between November and January. Spring semester online discussions 
lasted for 12 weeks; however, Week 6 was spring break, and there was either no or minimal participation 
in Weeks 7 and 8. Pre-service teachers were allowed to start their own discussion topics only in the spring 
semester. Table 1 presents a comparison of the nature of fall and spring semester online discussions. 
Further details on the tasks and participants’ evaluations of the course can be found in Satar and Akcan 
(2014). 
42 pre-service teachers took the fall semester course and 36 of them participated in online discussions. 
Four instructors acted as online facilitators. In the spring semester, 25 students took the course and 19 
students engaged with the online discussions. Two instructors acted as online facilitators. The instructors 
did not receive any training or guidelines for facilitation of online discussions. Three of them were 
experts in online communication. All pre-service teachers were Turkish and were 21–23 years old. While 
most participants used social media for informal communication, they were not active users of online 
professional networks and had little experience of participating in online formal or educational discussion 
forums. Participation in the fall semester course was a pre-requisite for registration in the spring semester 
course. Informed consent was gained prior to participation to the study and all personal details have been 
anonymized. 
Table 1. A Comparison of the Nature of Fall and Spring Semester Online Discussions 
 Fall Semester Discussions Spring Semester Discussions 
Topic Online tutoring skills by Hauck and 
Warnecke (2013) with a focus on SP and 
participatory literacy, pedagogical training, 
and technical training 
Extended practice for the use of Web 2.0 
tools (experimentation, creation, and 
evaluation) and extended discussions 
regarding teacher trainees’ face-to-face 
teaching experiences 
Tasks Highly structured in three stages (activating 
prior knowledge, learning about theory, and 
reflecting on experience), hence the tasks 
required some background reading before 
discussions 
Product-oriented tasks which did not require 
preparation; structured in two stages 
(introduction to tool or topic, sharing of 
products and experience) 
Student 
Discussion 
Students did not initiate any discussion 
topics 
Students initiated their own discussion topics 
Participants 36 pre-service teachers and 4 instructors 19 pre-service teachers and 2 instructors 
Grading Participation was graded (10% of overall 
course grade) 
Participation was graded (10% of overall 
course grade) 
Duration 9 weeks 12 weeks, de facto 9 weeks (Week 6, spring 
break, Weeks 7 and 8, no participation, as 
students had project submissions and 
midterm exams during those weeks) 
H. Müge Satar and Sumru Akcan 163 
 
Interaction Whole-group interaction (8 threads, Weeks 
1–7); interaction in three smaller groups (3 
threads, Weeks 7–9) 
Whole-group interaction throughout the 
semester (12 threads, 9 weeks) 
Data Collection and Analysis 
Data for this article were obtained from written records of pre-service teachers’ online discussions in fall 
and spring semester practicum courses. Participants’ online participation was analyzed by calculating 
instructors’ and pre-service teachers’ weekly number of forum entries and words they produced online in 
each semester (RQ1). 
Second, interaction patterns of pre-service teachers and instructors were examined both quantitatively and 
qualitatively (RQ2). For quantitative analysis, SNA was carried out to investigate the relations among 
participants of the online community. SNA uncovers “the social relations …, the structure of those 
relations, and how relations and their structures influence (or are influenced by) social behaviour, 
attitudes, beliefs, and knowledge” (Prell, 2012, p. 1). Data for SNA relate to “who speaks to whom in the 
online discussion” (Wu et al., 2014, p. 232). SNA was used to explore and compare interaction patterns in 
fall and spring semesters at actor level (for each participant) by calculating centrality scores and at group 
level (as a community) by calculating cohesiveness scores using UCINET 6 (Borgatti, Everett, & 
Freeman, 2002). 
In order to prepare the data for SNA, an interaction matrix was prepared calculating only the discussion 
entries participants posted in response to another participant’s forum entry. The entries in response to the 
main task or discussion topic were excluded at this stage. These responses to the main topic were 
accepted as an indication of participation to the course, but not of interaction. Moreover, data pertaining 
to the last three discussion threads of the fall semester (Weeks 7–9) were excluded. In these threads, 
participants were divided in three smaller groups. Although all participants could follow the discussions 
in any group, none posted any comments to the thread for another group. Therefore, given the potential 
effect of smaller groups on the development of relationships, data from these threads were not included in 
the interaction matrix. 
Another concern for SNA was the duration of the discussions. For comparative SNA, it is important to 
observe and compare interactions that take place in equal periods of time, because more time would mean 
higher opportunities for interaction. Therefore, as fall semester SNA results were calculated for the first 
six weeks of the interactions (as explained previously), spring semester SNA results had to be calculated 
also for interactions over six weeks to keep time constant and observe interactions within equal 
timeframes. Thus, interactions that took place in the first three weeks of the spring semester discussions 
were excluded in order to be able to observe any changes in the nature of the interactions during the initial 
six weeks with those during the final six weeks of the year. 
SNA was followed by a qualitative content analysis of interaction patterns. Qualitative content analysis is 
defined as “any qualitative data reduction and sense-making effort that takes a volume of qualitative 
material and attempts to identify core consistencies and meanings” (Patton, 2002, p. 453). In order to do 
this, the structures of all discussion threads were explored inductively in terms of which post was sent in 
response to what and when. Interaction patterns in two representative discussion threads from each 
semester were then identified. 
Third, SP levels of each participant was quantified using content analysis (RQ3) based on SP indicators 
(see Appendix A) developed by Rourke et al. (1999)1 and adapted by Swan and Shih (2005) and Shea et 
al. (2010). Content analysis involves identification of recurring trends and calculating frequencies (Shea 
et al., 2013). Following Rourke et al. (1999), the unit of analysis for quantitative content analysis was a 
theme or idea identified in the message. This provided flexibility to code a single discussion entry for 
more than one category where needed. After the data were coded, raw SP figures were calculated for each 
participant. Raw SP figures were then divided by the total number of words produced by each participant 
164 Language Learning & Technology 
 
and multiplied by 1000 in order to obtain a density index that indicated frequency of use per 1000 words 
(for the same approach, see Lomicka & Lord, 2007; Rourke et al., 1999). 
In order to ensure reliability, both researchers initially coded one third of the data individually. Before 
coding, the researchers met and studied the adapted version of SP by Shea et al. (2010).2 Two of the 
indicators of the interactive dimension (continuing a thread and quoting from others’ messages) were 
software dependent and were excluded, as they did not apply to the online platform used in this study. 
Based on the previous work of Satar (2015), a new category, intersubjectivity, was added to cohesive 
indicators. Intersubjectivity is “sharing of experiential content (e.g., feelings, perceptions, thoughts, and 
linguistic meanings) among a plurality of subjects” (Zlatev, Racine, Sinha, & Itkonen, 2008, p. 1). In this 
study, intersubjectivity is operationalized as the act of finding common ground by emphasizing or 
pointing out similarities between oneself and other participants. 
While one of the researchers completed coding on pen and paper, the other used the qualitative analysis 
software, Atlas-ti 7. The results of each coder were compared and calculated in percentages. The inter-
rater reliability was 53%. As this was a low figure, the researchers met to compare the codes before 
embarking on re-coding of the data.3 Significant discrepancies were found to be due to different methods 
of manual and software coding. Other differences in coding were also found in the coding of personal 
advice (interactive category) and social sharing (cohesive category). Swan and Shih (2005) define 
personal advice as “offering specific advice to classmates,” exemplified by “The CEC website might have 
some references” (p. 136). However, not all instances of advice in our data exactly matched this 
explanation. For example, some of the data coded as personal advice functioned as expression of potential 
future actions (e.g., … it seems like beneficial, we should just try [and see whether] it [will]work properly 
or not) or as providing examples (e.g., You can see an example of wordle word cloud here). Sometimes, 
the instances were implicit in the form of asking questions (e.g., I generally do not do it but have you 
tried it?). Moreover, in the spring semester, pre-service teachers initiated discussion threads stimulated by 
seeking advice, for example, for ensuring silence in the classroom. When providing advice, participants 
would describe an activity their cooperating teacher had used and end with an implicit advice: My 
cooperating teacher said this tool works almost every time. As neither Swan and Shih (2005) nor Shea et 
al. (2010) provide further guidance on coding or theoretical grounding for this category, and due to the 
complexity of identifying personal advice, we decided to remove it from the coding scheme. Another 
category we removed was social sharing. We realized that one of the coders had misinterpreted the 
indicator, and that the category did not exist in our data. Similarly, course reflection (cohesive category) 
did not apply to the data and was thus removed (see Appendix A). For the second coding, all data (a total 
of 62,223 words) were coded individually by each researcher using Atlas-ti 7. Following this second 
coding, inter-rater reliability was 97% and final coding was accepted following full agreement between 
the coders. 
Once data were coded and SP density scores were calculated for each participant, and in order to track 
change in the amount of SP, Wilcoxon Signed Rank tests were run comparing SP levels of the pre-service 
teachers who participated in both fall and spring semester online discussions. These included a 
comparison in terms of a total SP density score as well as density scores for each category of the SP 
framework (i.e., affective, interactive, and cohesive). 
Finally, in order to find out whether SNA centrality measures could be an indicator of SP (RQ4), 
Spearman Rho correlations were calculated between the SP measures (total SP, affective, interactive, and 
cohesive indicators) and centrality measures (network, outdegree, and indegree centrality) of participants 
in fall and spring semesters. These correlations were run separately for each semester because the number 
of participants in each semester was different. 
H. Müge Satar and Sumru Akcan 165 
 
Results 
RQ1: Participation 
This section seeks to find out participation levels of the instructors and pre-service teachers in fall and 
spring semester online forum discussions. Number of forum entries and number of words per week 
produced by the instructors and pre-service teachers are presented in Figure 1. Participation during the fall 
semester continued over nine weeks between November and December. During this time, 36 pre-service 
teachers and four instructors produced a total of 405 forum entries (43,708 words). The online component 
for the spring semester lasted from mid-March to the beginning of June, over a total of 12 weeks with a 3-
week interval. During the spring semester, 19 pre-service teachers and two instructors produced a total of 
244 forum entries (18,899 words). In terms of instructor participation, Figure 1 indicates that instructors 
were highly active with high online presence during both semesters of the course, amounting to about one 
third of the total participation. 
Figure 1 also shows that in the fall semester overall activity levels peaked during the initial weeks and 
then gradually decreased. This gradual decline in participation in the fall semester could be attributed to 
participants’ lack of willingness or time to read the background material prior to discussions. Another 
explanation could be due to the discussions being in small groups. Although pre-service teachers’ 
themselves requested smaller groups, their motivation to participate might have declined based on varying 
group dynamics. 
 
Figure 1. The number of forum entries and words per week. 
In terms of number of entries, instructor contribution did not fluctuate much throughout the semester. On 
166 Language Learning & Technology 
 
the other hand, pre-service teachers’ contribution peaked during Week 2, then decreased and stabilized 
during the rest of the semester. However, regarding number of words produced, instructors’ contribution 
peaked during Week 4, while pre-service teachers produced the lowest number of words in the same 
week. This can be explained by the requirements of the task. For Week 4 activities, pre-service teachers 
created an online poster reflecting their participation patterns. Therefore, discussion entries were either 
images or links to their posters on external sources, which were not counted in the number of words. Yet, 
instructors’ comments were verbal, interpreting and commenting on the visual elements of the posters. 
In the spring semester, pre-service teachers’ participation peaked on Weeks 2 and 10, in other words, at 
the beginning and the end of the semester. Total participation in terms of the number of entries posted 
also peaked on Week 5, mostly due to an increase in the instructors’ number of entries. Participation in 
the spring semester, especially of pre-service teachers, seemed to stay even with two or three peaks. This 
may indicate that pre-service teachers were able to transfer the knowledge and skills they practiced in the 
fall semester course—especially in terms of participation patterns—to their online participation practices 
in the spring semester course. 
It is also possible to interpret the fluctuations in participation patterns considering the nature and topic of 
the tasks. During the fall semester, highest pre-service teacher participation was observed during Weeks 
2, 4, and 8. The topic of the tasks for these weeks were introductions and discussion on the use of 
personal profiles using the online system, reflecting on and creating posters of participation patterns, and 
watching a talk on online education and reflecting on the ideas in the talk, respectively. The greatest 
amount of pre-service teacher participation during the spring semester was observed on Weeks 2 and 10. 
The topic of the discussion for Week 2 was producing and sharing a task for the first weeks of an online 
course. Forum discussions during Week 10 centered around three threads. The first one was a task 
assigned by the instructors asking pre-service teachers to create and share an online story book. Pre-
service teachers initiated the other two threads, which focused on classroom management techniques. 
They asked advice on and shared their experiences of how they ensured silence and smooth transition 
between the activities while teaching in their practicum schools. Therefore, it is also possible to attribute a 
more stable participation throughout the spring term to the fact that the participants were allowed to 
initiate their own discussion topics on issues of immediate relevance. The instructor-assigned topics in the 
fall semester training were on online teaching, a topic that was highly relevant for future teaching 
practices of the participants, but that had limited immediate relevance. Moreover, in both semesters, pre-
service teachers’ participation increased when the topics focused on the design, production, and sharing of 
online tools and tasks. 
RQ2: Interaction 
This section reports on the results on interaction patterns obtained from SNA followed by the results of a 
qualitative analysis of two representative discussion threads. 
Actor Level Interaction (SNA) 
In SNA, the amount of interaction an individual has with other members in a social network is measured 
by calculating centrality scores (Wasserman & Faust, 1994). Degree centrality is “the number of 
immediate contacts an actor has in a network” (Prell, 2012, p. 97) and indicates an individual’s 
involvement or activity in a network. Degree centrality measures are dependent on the number of people 
(i.e., actors) in a group. When group sizes differ, Freeman’s normalized centrality measures are 
recommended to compare the activity levels of the same actor in groups that have different number of 
participants (see Prell, 2012). 
The normalized degree, indegree, and outdegree centrality measures for the fall semester (see Appendix 
B) indicated that the most active participants in the online component were three of the instructors 
(represented with numbers 38, 40, and 37). These instructors had the highest degree, outdegree, and 
indegree centrality scores, which indicated that they were highly influential and popular central 
participants in the group. The fourth instructor (represented by number 36 in Appendix B), on the other 
H. Müge Satar and Sumru Akcan 167 
 
hand, did not participate much with zero centrality in all three measures. The mean normalized degree, 
outdegree, and indegree scores for the fall semester were 7.56, 5.06, and 5.06 respectively. Out of the 36 
pre-service teachers in the group, 10 had normalized degree, 7 had normalized outdegree, and 18 had 
normalized indegree centrality scores above the mean normalized degree scores. Six pre-service teachers 
were highly active in the group with centrality scores above the mean score in all three measures. These 
were the participants represented with numbers 2, 3, 4, 10, 12, and 28. In the fall semester online 
interactions, there were nine isolates (i.e., participants who did not send or receive direct responses to and 
from other participants). 
Spring semester normalized degree, outdegree, and indegree scores for each participant can be found in 
Appendix B. Accordingly, both instructors were again the most central participants with highest degree 
and outdegree measures. Yet, two of the participants, represented with numbers 18 and 10, were also 
quite central with high degree (45 and 40, respectively), outdegree (35 and 25, respectively), and indegree 
(35 and 30, respectively) measures. These measures were almost as high as or greater than one of the 
instructor’s centrality measures (represented by number 39). In the spring semester, the mean normalized 
degree, outdegree, and indegree scores were 22.38, 14.76, and 14.76 respectively, which were 
comparatively higher than the fall semester measures. Out of the 19 pre-service teachers in the group, six 
had normalized degree, four had normalized outdegree, and six had normalized indegree centrality scores 
above the mean normalized degree scores. Two pre-service teachers were highly active in the group with 
centrality scores above the mean score in all three measures. These were the participants represented with 
numbers 10 and 18. There were no isolates during the spring semester—in other words, all participants 
had at least one direct communication with another member of the group. 
168 Language Learning & Technology 
 
Fa
ll 
Se
m
es
te
r 
So
ci
og
ra
m
 
 
Sp
ri
ng
 S
em
es
te
r 
So
ci
og
ra
m
 
  
Figure 2. Fall and spring semester SNA results (squares represent instructors; circles represent pre-service 
teachers). Same numbers in each sociogram refer to the same participants. Line thickness represents the 
strength of the relationships. The arrows show the direction of the posts. Higher-resolution images of the 
fall semester sociogram and the spring semester sociogram are available online. 
Figure 2 demonstrates the relationships during fall and spring semesters as sociograms. Participants 10 
and 18 were highly active in both semesters, while other participants (especially 15, 17, 20, and 22) 
increased their involvement in the community and became more active participants in the spring semester. 
In the spring semester, the differences between normalized degree, outdegree, and indegree scores of the 
instructors and pre-service teachers were smaller, and there were no isolates. While these findings can 
represent the development of interaction and involvement throughout the courses, smaller group size in 
the spring semester can also account for improved interaction among the members. 
H. Müge Satar and Sumru Akcan 169 
 
Group Level Interaction (SNA) 
Several measures for network cohesion are presented in Table 2 for both fall and spring semesters. The 
measures were calculated for directed and undirected data. The calculation on directed data considered 
the direction of the relations (whether sending or receiving), leading to different levels of density and 
different scores for outdegree and indegree centrality measures. Density of a network represents the 
proportion of ties that are actually present compared to the number of all potential ties. The density score 
represents how strongly the participants in a network are linked together. Thus, a network with a higher 
density score is accepted to be more cohesive (Prell, 2012). As such, in terms of density figures, the 
spring semester network (0.15 and 0.22) seems to be a more cohesive network than the one in the fall 
semester (0.05 and 0.08). However, when interpreting density scores, a consideration of network size, 
centralization, and subgroups is also needed. 
Table 2. Network Cohesion During the Fall and Spring Semesters 
 Fall Directed Fall Undirected Spring Directed Spring Undirected 
Nodes (Participants) 40 40 21 21 
Degree Centralization 0.54 0.51 0.67 0.64 
Out-Central 0.53  0.50 0.63 0.61 
In-Central 0.24 0.50 0.32 0.61 
Density  0.05 0.08 0.15 0.22 
Components  22  10 6 1 
Connectedness  0.37 0.60 0.78 1.00 
Fragmentation  0.63 0.40 0.22 0.00 
Average Distance  2.37 2.18 2.15 1.97 
Diameter 5  4 4 4 
Compactness 0.19 0.31 0.42 0.58 
Regarding network size, smaller networks can have higher density scores because there are fewer 
potential ties, and hence the chances that the network will reach full potential are higher. Although 
density scores of the spring semester network seem to indicate a more cohesive community, the size of 
the spring semester network is also smaller (21 compared to 40). Therefore, other measures need to be 
compared. 
First, “because density is based on how many ties are present in the network, one or two individuals 
having a disproportionately high number of ties to others in the network might raise the density score” 
(Prell, 2012, p. 168). In other words, a high network density could be due to a high centralization score. 
Directed (0.67) and undirected (0.64) centralization scores in the spring semester were slightly higher 
than those of the fall semester (0.54 and 0.51, respectively). Therefore, although the difference is small, 
higher density levels in the spring semester might be partly due to higher centralization scores. Moreover, 
diameter “refers to the shortest path between two actors” (Prell, 2012, p. 171). A smaller diameter means 
that participants in a network are close to each other. The diameter of the spring semester network is 4 
based on both directed and undirected data, compared to the fall semester, with network diameters of 5 
and 4, respectively. Given small differences in centralization and diameter scores, these measures do not 
offer conclusive insights. 
Second, “a high network density score can easily result from many subgroups … large networks have 
fewer cohesive subgroups, and hence less amount of fragmentation” (Prell, 2012, p. 171). During the fall 
semester, there were 10 subgroups and fragmentation was 40% (0.40), whereas there was one subgroup 
during the spring semester with 0% fragmentation. The spring semester participants were also more 
170 Language Learning & Technology 
 
connected (0.78 and 1.00) than the fall semester participants (0.37 and 0.60). Therefore, in terms of 
components, fragmentation, and connectedness, smaller network size does not seem to affect higher 
network density, and thus cohesion, in the spring semester interactions. 
Therefore, considering network sizes, density and centralization scores, number of components, 
fragmentation scores, and diameter of the networks, as well as higher connectedness and compactness 
with slightly lower average distance between the participants, interactions during the spring semester 
appeared to be more cohesive than the interactions during the fall semester. Given actor-level SNA 
results, it is possible to argue that the pre-service teachers were more involved in the discussions in the 
spring semester. However, higher amounts of interaction and the establishment of a more cohesive 
network during the spring semester could be better explained by a closer, qualitative investigation of 
representative forum threads from each semester. 
Qualitative Analysis for Interaction Patterns 
This section explores interaction patterns in each semester qualitatively by focusing on and comparing 
structures of two representative forum discussion threads. Extract 1 and Extract 2 illustrate typical 
interaction patterns in the fall and spring semesters. Each line indicates a forum entry and replies to each 
entry are indented toward the right. For instance, in Extract 1, Line 6 is a response to Line 1, Line 7 to 
Line 6, Line 8 to Line 7, and Line 9 is a response to Line 1. The discussion thread in Extract 1 was chosen 
because it occurred predominantly during Weeks 4 and 5, when the participants had already gotten used 
to the blended environment and the online tutoring skills training. There was also a high amount of 
participation in this thread. Extract 2 represents one such thread on the topic of classroom management 
techniques. This extract is also from a thread with a high amount of participation. 
 
Extract 1. Fall semester interaction patterns for Weeks 4 and 5, Task 2-2, creating an online poster; T = 
instructor, S = pre-service teacher. 
H. Müge Satar and Sumru Akcan 171 
 
 
Extract 2. Spring semester interaction patterns, thread initiated by a trainee on classroom management 
techniques; T = instructor, S = pre-service teacher. 
During fall semester, forum discussions were characterized by pre-service teachers’ response to the 
instructor’s initial post (discussion trigger, or initiation), feedback from the instructor to pre-service 
teacher’s response, and a limited number of pre-service teachers posting responses to the instructor’s 
feedback. This interaction closely resembled the teacher initiation–student response–teacher feedback 
(IRF) sequence with an optional student response to teacher feedback observed in classroom research. 
This pattern is exemplified in sample posts in Appendix C, Sample 1, representing Lines 16, 17, and 18 of 
Extract 1. There was also some initial evidence of student–student interaction (Extract 1, Lines 26 and 27 
and Lines 31 and 32). In the spring semester, the interaction pattern in the first discussion thread 
resembled the patterns in the fall semester. However, as the discussions moved on, instructors took on a 
less-active role with increased evidence of student–student interaction (Extract 2, Lines 9, 10, 11, and 13; 
Lines 14, 16, and 17; Lines 21, 28, and 29; and Lines 30, 31, and 34). These examples indicate more 
complex, pre-service-teacher oriented interaction patterns in the spring semester (for sample posts 
representing Lines 30–34, see Appendix C, Sample 2). 
RQ3: Social Presence 
In order to investigate differences in participants’ SP levels over two semesters, raw SP and density levels 
were calculated (see Appendix D). Wilcoxon Signed Rank tests were then run comparing density levels 
for each semester. As shown in Table 3, results of the Wilcoxon Signed Ranks test indicated that there 
were no significant differences between the semesters for pre-service teachers’ overall SP density levels 
(Z =  
-1.01, p = .314) or for cohesive indicators (Z = -0.36, p = .717). However, pre-service teachers’ affective 
indicator densities in the spring semester were significantly lower (Z = -2.17, p < .005), while their 
interactive densities in the spring semester were significantly higher (Z = -3.574, p < .001) than that of 
their fall semester affective and interactive densities.  
172 Language Learning & Technology 
 
Table 3. Wilcoxon Signed Rank Test for Related Measures for SP Density Levels of Pre-Service Teachers 
in the Fall and Spring Semesters 
Fall–Spring Ranks N Mean Rank Sum of Ranks Z p (2-Tailed) 
SP Total Negative 10a 7.78 70.00 -1.006e .314 
 Positive 9b 12.00 120.00   
 Ties 0c     
Affective Negative 14a 10.64 149.00 -2.173d .030* 
 Positive 5b 8.20 41.00   
 Ties 0c     
Cohesive Negative 9a 9.56 86.00 -0.362e .717 
 Positive 10b 10.40 104.00   
 Ties 0c     
Interactive Negative 1a 1.00 1.00 -3.574e .000** 
 Positive 16b 9.50 151.00   
 Ties 2c     
*p < .05, **p < .01 
aspring < fall 
bspring > fall 
cspring = fall 
dBased on positive ranks 
eBased on negative ranks 
RQ4: Social Presence and Social Network Analysis 
A final area of investigation in this study was to explore whether participants’ (both pre-service teachers’ 
and instructors’) SP levels and interaction patterns based on SNA analysis (i.e., degree, indegree, and 
outdegree centrality measures) were related. For this purpose, total SP density levels and SP density 
levels for each category—as well as degree, indegree, and outdegree centrality measures for all 
participants—were calculated for each semester separately. Spearman Rho correlations were then 
calculated using these measures (see Table 4). 
Spearman’s rank order correlations indicated that in the fall semester there were strong, positive, and 
statistically significant correlations between interactive density scores and centrality (rs(40) = .62, p 
= .000), prestige (rs(40) = .58, p = .000), and influence (rs(40) = .77, p = .000) measures. A weak, positive 
relationship was also found between cohesive density and influence (rs(40) = .37, p = .019). However, in 
the spring semester, no significant relationships were found between SP and SNA measures. The 
existence of negative and weak (fall semester) and almost non-existent (spring semester) correlations 
between affective indicators of SP and centrality, prestige, and influence is worth noting.  
H. Müge Satar and Sumru Akcan 173 
 
Table 4. Spearman Rho Correlation Coefficients Between SNA and SP Density Measures in the Fall and 
Spring Semesters 
 
Centrality 
(Degree) 
Prestige 
(Indegree) 
Influence 
(Outdegree) 
SP 
Total Affective Cohesive Interactive 
Centrality (Degree) - .970** .825** .217 -.249 .285 .621** 
Prestige (Indegree)  - .728** .238 -.194 .280 .578** 
Influence (Outdegree)   - .286 -.287 .368* .766** 
SP Total    - .637** .893** .433** 
Affective     - .340* -.078 
Cohesive      - .423** 
Interactive       - 
        
Centrality (Degree) - .934** .773** .094 .049 .225 -.025 
Prestige (Indegree)  - .625** -.138 .018 .111 -.245 
Influence (Outdegree)   - .227 .051 .182 .230 
SP Total    - .373 .617** .867** 
Affective     - .418 .135 
Cohesive      - .373 
Interactive       - 
*p < .05, **p < .01 
Discussion 
This article investigated pre-service teachers’ participation, interaction patterns, and SP in online 
components of two blended practicum courses offered over two semesters of one academic year. It also 
explored the relationship between outcomes of SNA and content analysis as ways of accounting for SP. 
First, our analysis in relation to participation and interaction patterns revealed that while participation in 
the fall semester gradually decreased toward end of the term, more stable and consistent participation was 
observed throughout the spring semester. Other studies have found increasingly continuous participation 
and engaging discussions over time (Hara et al., 2000; Wu et al., 2014; Zheng and Warschauer, 2015). 
However, the effects of time on participation seemed to be less influential in our analysis. Rather, our 
results indicated a potential transfer of skills presented and practiced in the online component during fall 
semester to spring semester participation. Results of actor-level and group-level SNA, as well as 
qualitative analysis of two discussion threads, indicated a more cohesive group in the spring semester 
with higher amount of interaction, greater connectedness, and fewer subgroups. There were no isolates in 
the spring semester discussions, and interaction patterns of some pre-service teachers began to resemble 
that of the instructors as they began to provide feedback on each other’s posts. Pre-service teachers with 
low influence and popularity in the fall semester became more active participants of the community in the 
spring semester. Increased interaction observed in the spring semester could mean that although outcomes 
of online tutoring skills training were not immediately visible during the fall semester, acquisition of 
these skills were observed in further opportunities for online interaction. This has implications for 
research in evaluating the success of teacher training and skills, suggesting the need for longitudinal 
studies on teachers’ and pre-service teachers’ participatory skills. Moreover, as Wu et al. (2014) suggest, 
time is a potentially significant contributor to change in interactional patterns of teachers. Therefore, 
learning to participate actively in professional communities may take time. Incorporating greater 
174 Language Learning & Technology 
 
opportunities for blended learning in EFL teacher education would perhaps aid pre-service teachers in 
experimenting with and advancing their interaction and participation skills. 
Another factor that could have played a role in the varying levels of participation and interaction over the 
two semesters could be the nature and topic of the tasks. Baek and Kim (2015) reported that nature of 
discussion topics affected participants’ interaction patterns. In this study, pre-service teachers were 
allowed to initiate their own discussion topics only during the spring semester. Moreover, spring semester 
topics were more immediately relevant as they dealt with classroom teaching issues, which could have 
empowered the pre-service teachers and increased feelings of ownership of the platform. Tasks that 
required design, production, and sharing were also popular and elicited an increased amount of 
participation in both semesters. Exploring the effects of topics and tasks on participation and SP is still an 
under-researched area. 
Second, high instructor participation and online presence was observed in both semesters. Instructors 
were highly active and were key players in the group as central actors, modeling necessary behaviors for 
participation in online communities. While Szeto (2015) and Pawan et al. (2003) underscore the 
importance of instructor involvement to encourage learner participation, Shea et al. (2010) argue that 
strategic instructors do not necessarily have to be the most active participants and that network prestige 
(i.e., the number of messages instructors receive) could be a more appropriate indicator of posts directed 
at course requirements, eliciting a high amount of responses from the learners. Similarly, it is possible to 
argue that while highly active instructors may help model certain participation skills, they may at the 
same time dominate the discussion and not leave much room or need for student participation or 
intimidate less-active learners. Future research about the effects of instructors’ presence and participation 
on those of the learners could yield significant outcomes. 
Third, Zhao et al. (2014) argue for an optimal level of SP to achieve online participation and 
collaboration. Our comparison of SP over two semesters demonstrated that overall SP density levels and 
the amount of cohesive indicators of the pre-service teachers remained stable. Affective indicators 
decreased, while interactive indicators increased. A decrease in the affective indicators over time is in line 
with previous research (Shea et al., 2010; Vaughan & Garrison, 2006) and is an indication that the 
participants established camaraderie and did not need further affective interaction to sustain collaboration. 
Finally, SNA has been found to be a potentially powerful tool to investigate interaction and an alternative 
method to measure SP as high levels of SP were observed in contributions of learners who were central to 
the network (Shea et al., 2010, 2014). Yet, Wu et al. (2014) observed a higher amount of SP indicators in 
the posts of peripheral members of a professional teachers’ community. We found statistically significant 
relationships between all SNA measures and interactive indicators of SP in the fall semester, which meant 
that central participants with high influence and prestige used more of the interactive indicators of SP in 
their posts (i.e., they asked questions; referred to other participants’ messages; and showed appreciation 
of, agreement with, and disagreement with other messages). However, the lack of a similar pattern in the 
spring semester and the presence of minimal correlations between SNA measures and affective indicators 
of SP call for further investigation into the relationship between SNA and content analysis for SP, 
especially in examining affective interaction. Moreover, qualitative investigations of participation, 
interaction, and SP in online communities could offer convincing and corroborating evidence. 
Conclusion 
Totally or partially online teaching activities are increasingly becoming an integral part of EFL 
instruction. In these environments, teachers need to motivate learners to participate online, regulate 
learner socio-emotional involvement, and facilitate community building in order to foster online 
interaction and collaboration (Coleman, Hampel, Hauck, & Stickler, 2012). Coleman et al. (2012) 
maintain that online teachers “need to be technically literate, employ tools best suited for the task, 
moderate activities, provide careful scaffolding of tasks, and give detailed instructions” (p. 173). That is 
H. Müge Satar and Sumru Akcan 175 
 
why online teaching skills training, especially through personal experience and reflective practice, should 
become an essential component of EFL teacher training. 
In this article, we investigated participation, interaction patterns, and SP of EFL pre-service teachers in 
two online discussion boards as part of two blended practicum courses. Our findings pertain to the context 
in which the blended courses were offered. It was impossible to anticipate to what extent participant 
interactions were shaped by the dynamics of face-to-face interactions, because only contributions to the 
online component were investigated. Therefore, future directions for research in blended contexts should 
include a study of interactions in both face-to-face and online components. Another potential direction for 
research is to explore the effects of instructor presence and involvement on learner participation, 
interaction, and SP. The potential of SNA in investigating the affective dimension of SP in online 
contexts also merits further investigation. 
Our results demonstrate that the online training offered in the fall semester practicum course on SP and 
participatory literacy assisted pre-service teachers in becoming more active members of the online 
community with more stable participation patterns and increased interaction in a more cohesive 
community. As the discussions progressed, the focus shifted from more affective to more interactive 
exchanges, implicating the establishment of a sense of camaraderie with increasing level of collaboration. 
We also examined the potential of SNA as an alternative method for the measurement of SP and found 
SNA to be an analytically powerful tool, especially for the investigation of the interactive dimension of 
the SP framework in the fall semester. Finally, while the CoI framework treats SP as a quality of the 
group, SNA methods that explore SP treat it as a quality of individuals in the network. In this respect, the 
latter has greater potential to describe SP as a dynamic quality of individuals, making SNA analysis a 
potentially unique and powerful method in the investigation of SP. 
Acknowledgements 
This work was supported by Boğaziçi University Research Fund Grant No. 6957. We would like to thank 
the anonymous reviewers and the editors of this issue for their insightful comments and recommendations 
on earlier versions of this article. 
Notes 
1. For a retrospective on the CoI framework with a discussion on validity, see article by Garrison, 
Anderson, and Archer (2010). 
2. This adaptation of the CoI framework was specifically chosen to produce comparable results for the 
relationship between SNA and content analysis (see RQ4). 
3. We observed that a remarkable amount of variance between the researchers’ coding was due to 
different ways of coding and quantifying the codes (i.e., use of software vs. manual coding). 
Moreover, the researcher who used software for content analysis had used the coding scheme in her 
earlier work and was thus more experienced. The second researcher, on the other hand, was using the 
coding scheme for the first time. Therefore, the first coding procedure functioned like a training 
period. We found that this was resolved upon a second coding of the data. 
References 
Arnold, N., & Ducate, L. (2006). Future foreign language teachers’ social and cognitive collaboration in 
an online environment. Language Learning & Technology, 10(1), 42–66. 
https://dx.doi.org/10125/44046   
Arnold, N., Ducate, L., Lomicka, L., & Lord, G. (2005). Using computer-mediated communication to 
establish social and supportive environments in teacher education. CALICO Journal, 22(3), 537–566. 
176 Language Learning & Technology 
 
Aviv, R., Erlich, Z., Ravid, G., & Geva, A. (2003). Network analysis of knowledge construction in 
asynchronous learning networks. Journal of Asynchronous Learning Networks, 7(3), 1–23. 
Baek, S. I., & Kim, Y. M. (2015). Longitudinal analysis of online community dynamics. Industrial 
Management & Data Systems, 115(4), 661–677. 
Borgatti, S. P., Everett, M. G., & Freeman, L. C. (2002). Ucinet for Windows: Software for social network 
analysis. Harvard, MA: Analytic Technologies. 
Çelik, S. (2013). Unspoken social dynamics in an online discussion group: The disconnect between 
attitudes and overt behavior of English language teaching graduate students. Educational Technology 
Research and Development, 61(4), 665–683. 
Coleman, J. A., Hampel, R., Hauck, M., & Stickler, U. (2012). Collaboration and interaction: the keys to 
distance and computer-supported language learning. In G. S. Levine, A. Phipps, & C. Blythe (Eds.), 
Critical and intercultural theory and language pedagogy (pp. 161–180). Florence, KY: Cengage 
Learning. 
Creswell, J. W. (2007). Qualitative inquiry & research design: Choosing among five approaches (2nd 
ed.). Thousand Oaks, CA: Sage.  
Dooly, M., & O’Dowd, R. (2012). Researching online foreign language interaction and exchange: 
Theories, methods, and challenges. Berlin, Germany: Peter Lang. 
Duensing, A., Stickler, U., Batstone, C., & Heins, B. (2006). Face-to-face and online interactions - Is a 
task a task? Journal of Learning Design, 1(2), 35–45. 
Farrell, C. S. T. (2007). Reflective language teaching: From research to practice. New York, NY: 
Continuum. 
Freese, A. R. (2006). Reframing one’s teaching: Discovering our teacher selves through reflection and 
inquiry. Teaching and Teacher Education, 22(1), 100–119. 
Garrison, D. R., & Arbaugh, J. B. (2007). Researching the community of inquiry framework: Review, 
issues, and future directions. The Internet and Higher Education, 10(3), 157–172. 
Garrison, D. R., & Kanuka, H. (2004). Blended learning: Uncovering its transformative potential in 
higher education. Internet and Higher Education, 7, 95–105. 
Garrison, D. R., Anderson, T. & Archer, W. (1999). Critical inquiry in a text-based environment: 
Computer conferencing in higher education. The Internet and Higher Education, 2(2–3), 87–105. 
Garrison, D. R., Anderson, T., & Archer, W. (2010). The first decade of the community of inquiry 
framework: A retrospective. The Internet and Higher Education, 13(1-2), 5–9. 
Garrison, D. R., Cleveland-Innes, M., & Fung, T. S. (2010). Exploring causal relationships among 
teaching, cognitive, and social presence: Student perceptions of the community of inquiry framework. 
The Internet and Higher Education, 13(1), 31–36. 
Gillham, B. (2000). Case study research methods. London, UK: Continuum. 
Gunawardena, C. N., & Zittle, F. J. (1997). Social presence as a predictor of satisfaction within a 
computer-mediated conferencing environment. American Journal of Distance Education, 11(3), 8–26. 
Hara, N., Bonk, C. J., & Angeli, C. (2000). Content analysis of online discussion in an applied 
educational psychology course. Instructional Science, 28(2), 115–152. 
Hauck, M., & Warnecke, S. (2013). Materials design in CALL: Social presence in online environments. 
In T. Michael, H. Reinders, & M. Warschauer (Eds.), Contemporary computer-assisted language 
learning (pp. 95–115). London, UK: Bloomsbury. 
H. Müge Satar and Sumru Akcan 177 
 
Hoven, D. (2006). Designing for disruption: Remodelling a blended course in technology in (language) 
teacher education. In L. Markauskaite, P. Goodyear, & P. Reimann (Eds.), Proceedings of the 23rd 
annual ascilite conference: Who’s learning? Whose technology? (pp. 339–349). Sydney, Australia: 
University of Sydney. 
Kehrwald, B. (2010). Being online: Social presence as subjectivity in online learning. London Review of 
Education, 8(1), 39–50. 
King, K., & Ellis, T. J. (2009). Comparison of social presence in voice-based and text-based 
asynchronous computer conferences. In Proceedings of the 42nd Hawaii International Conference on 
System Sciences (pp. 1–10). Los Alamitos, CA: IEEE Computer Society. 
Lim, D. H., & Morris, M. L. (2009). Learner and instructional factors influencing learning outcomes 
within a blended learning environment. Educational Technology & Society, 12(4), 282–293. 
Lomicka, L., & Lord, G. (2007). Social presence in virtual communities of foreign language (FL) 
teachers. System, 35(2), 208–228. 
Loughran, J. J. (2002). Effective reflective practice: In search of meaning in learning about teaching. 
Journal of Teacher Education, 53(1), 33–43. 
Lowenthal, P. R. (2012). Social presence: What is it? How do we measure it? (Unpublished doctoral 
dissertation). University of Colorado Denver, Denver, CO. 
Lowenthal, P. R., & Dunlap, J. C. (2014). Problems measuring social presence in a community of inquiry. 
E-Learning and Digital Media, 11(1), 19–30. 
Meskill, C. (2009). CMC in language teacher education: Learning with and through instructional 
conversations. International Journal of Innovation in Language Learning and Teaching, 3(1), 51–63. 
Michinov, N., & Michinov, E. (2008). Face-to-face contact at the midpoint of an online collaboration: Its 
impact on the patterns of participation, interaction, affect, and behavior over time. Computers & 
Education, 50, 1540–1557. 
Murphy, E. (2004). Recognising and promoting collaboration in an online asynchronous discussion. 
British Journal of Educational Technology, 35(4), 421–431. 
O’Toole, J. M., & Absalom, D. J. (2003). The impact of blended learning on student outcomes: Is there 
room on the horse for two? Journal of Educational Media, 28(2–3), 179–190. 
Patton, M. Q. (2002). Qualitative research and evaluation methods. Thousand Oaks, CA: Sage. 
Pawan, F., Paulus, T. M., Yalcin, S., & Chang, C.-F. (2003). Online learning: Patterns of engagement and 
interaction among in-service teachers. Language Learning & Technology, 7(3), 119–140. 
https://dx.doi.org/10125/25217   
Prell, C. (2012). Social network analysis: History, theory, and methodology. London, UK: Sage. 
Reffay, C., & Chanier, T. (2003). How social network analysis can help to measure cohesion in 
collaborative distance-learning. In B. Wasson, S. Ludvigsen, U. Hoppe (Eds.), Designing for change 
in networked learning environments: Proceedings of the International Conference on Computer 
Support for Collaborative Learning 2003, (pp. 343–352). Bergen, Norway: Kluwer Academic 
Publishers. 
Remesal, A., & Colomina, R. (2013). Social presence and online collaborative small group work: A 
socioconstructivist account. Computers & Education, 60, 357–367. 
Richardson, W. (2006). Blogs, wikis, podcasts, and other powerful web tools for classrooms. Thousand 
Oaks, CA: Corwin Press. 
178 Language Learning & Technology 
 
Rourke, L., Anderson, T., Garrison, D. R., & Archer, W. (1999). Assessing social presence in 
asynchronous text-based computer conferencing. The Journal of Distance Education, 14(2), 50–71. 
Satar, H. M. (2007). Task-based language learning in written synchronous computer mediated 
communication. (Unpublished master’s dissertation). The Open University, Milton Keynes, UK. 
Satar, H. M. (2015). Sustaining multimodal language learner interactions online. CALICO Journal, 32(3), 
480–507. 
Satar, H. M., & Akcan, S. (2014). Pre-service language teachers’ reflections on the implementation of a 
blended-learning environment. Turkish Online Journal of Qualitative Inquiry, 5(3), 42–61. Retrieved 
from http://dergipark.ulakbim.gov.tr/tojqi/article/view/5000093512/5000087044 
Scott, J. (2013). Social network analysis (3rd ed.). London, UK: Sage. 
Shea, P., Hayes, S., Uzuner-Smith, S., Gozza-Cohen, M., Vickers, J., & Bidjerano, T. (2014). 
Reconceptualizing the community of inquiry framework: An exploratory analysis. Internet and 
Higher Education, 23, 9–17. 
Shea, P., Hayes, S., Uzuner-Smith, S., Vickers, J., Bidjerano, T., Gozza-Cohen, M., Jian, S., Pickett, A. 
M., Wilde, S., & Tseng, C. (2013). Online learner self-regulation: Learning presence viewed through 
quantitative content- and social network analysis. The International Review of Research in Open and 
Distance Learning, 14(3), 427–461. 
Shea, P., Hayes, S., Vickers, J., Gozza-Cohen, M., Uzuner, S., Mehta, R., Valchova, A., & Rangan, P. 
(2010). A re-examination of the community of inquiry framework: Social network and content 
analysis. Internet and Higher Education, 13, 10–21. 
Sing, C. C., & Khine, M. S. (2006). An analysis of interaction and participation patterns in online 
community. Educational Technology & Society, 9(1), 250–261. 
Swan, K., & Shih, L. F. (2005). On the nature and development of social presence in online course 
discussions. Journal of Asynchronous Learning Networks, 9(3), 115–136. 
Szeto, E. (2015). Community of inquiry as an instructional approach: What effects of teaching, social, and 
cognitive presences are there in blended synchronous learning and teaching? Computer & Education, 
81, 191–201. 
Tsai, I.-C. (2012). Understanding social nature of an online community of practice for learning to teach. 
Educational Technology & Society, 15(2), 271–285. 
Tu, C., & McIsaac, M. (2002). The relationship of social presence and interaction in online classes. 
American Journal of Distance Education, 16(3), 131–150. 
Vaughan, N., & Garrison, D. R. (2006). How blended learning can support a faculty development 
community of inquiry. Journal of Asynchronous Learning Networks, 10(4), 139–152. 
Vygotsky, L. S. (1978). Mind in society. Cambridge, MA: MIT Press. 
Wasserman, S., & Faust, K. (1994). Social network analysis: Methods and applications. Cambridge, UK: 
Cambridge University Press. 
Wegerif, R. (1998). The social dimension of asynchronous learning. Journal of Asynchronous Learning 
Networks, 2(1), 34–49. 
Whiteside, A. L. (2015). Introducing the social presence model to explore online and blended learning 
experiences. Online Learning Journal, 19(2). Retrieved from 
http://olj.onlinelearningconsortium.org/index.php/olj/article/view/453/137 
H. Müge Satar and Sumru Akcan 179 
 
Wu, H., Gao, J., & Zhang, W. (2014). Chinese EFL teachers’ social interaction and socio-cognitive 
presence in synchronous computer-mediated communication. Language Learning & Technology, 
18(3), 228–254. https://dx.doi.org/10125/44392  
Yin, R. K. (2003). Case study research: Design and methods (3rd ed.). Thousand Oaks, CA: Sage. 
Yuan, J., & Kim, C. (2014). Guidelines for facilitating the development of learning communities in online 
courses. Journal of Computer Assisted Learning, 30, 220–232. 
Zhao, H., Sullivan, K. P. H., & Mellenius, I. (2014). Participation, interaction, and social presence: An 
exploratory study of collaboration in online peer review groups. British Journal of Educational 
Technology, 45(5), 807–819. 
Zheng, B., & Warschauer, M. (2015). Participation, interaction, and academic achievement in an online 
discussion environment. Computers & Education, 84, 78–89. 
Zlatev, J., Racine, T. P., Sinha, C., & Itkonen, E. (2008). Intersubjectivity: What makes us human? In J. 
Zlatev, T. P. Racine, C. Sinha, & E. Itkonen (Eds.), The shared mind: Perspectives on 
intersubjectivity (pp. 1–14). Amsterdam, Netherlands: John Benjamins. 
Appendix A. Coding Scheme for Social Presence 
Adapted from Rourke et al. (1999), Swan and Shih (2005), and Shea et al. (2010) 
SP Categories Indicators Definition Examples 
Affective Expressing 
emotions 
Conventional 
expressions of emotion  
Therefore, I am happy to be a part of 
it. // It is definitely exciting to read 
how people label themselves … 
 Use of humor Teasing, cajoling, 
irony, sarcasm, 
understatements 
I could have used a ‘like’ button 
here;) 
 Self-disclosure Presents details of life 
outside of class, or 
expresses 
vulnerability; includes 
expressions of likes, 
dislikes and 
preferences 
First of all, it is another good 
opportunity for such a non-
technologic person as me to 
experience web-based sharing 
platform 
 Emoticons, 
punctuation, etc. 
Unconventional 
expressions of 
emotion: repetitious 
punctuation, 
conspicuous 
capitalization, 
emoticons 
Otherwise, it does not go further 
beyond just doing homeworks, 
unfortunately. =/ 
// That is a VERY important thing 
 Expressing value Expressing personal 
values, beliefs and 
attitudes 
I find a profile is useful to express my 
ideas, but I like limitations blocking 
to share all my personality. 
  
180 Language Learning & Technology 
 
Cohesive Vocatives Addressing by name  I am also from Mersin like Ayse :) 
 Inclusive pronouns Addresses the group as 
we., us, our, group 
Hi my dear friends, our dear 
instructor and our coordinating 
instructors, 
 Phatics, salutations Communication that 
serves a purely social 
function; greetings or 
closures 
Hey! // See you guys! 
 Intersubjectivity Finding common 
ground by 
emphasizing 
similarities 
I do like this one a lot too. // And I 
seem to be like you, for example, I 
can remember what a webpage looked 
like more than the name of the page. 
Interactive Referring to other’s 
messages 
Direct references to 
contents of others' 
posts 
Most of my friends have written about 
the drawback of profiles // This is a 
really nice idea 
 Asking questions Asking questions to 
other participants 
What about turning on or off lights? I 
generally do not do it but have you 
tried it? 
 Complimenting, 
appreciation 
Complimenting others 
or contents of others' 
messages 
Thank you for the videos, I’ll also 
watch them :) // Thanks aylin these 
are very useful tips :)) 
 Expressing 
agreement 
Expressing agreement 
with others or contents 
of others’ messages 
I agree with you, the student focuses 
more on giving points than on the 
lesson in your case. // Exactly! ;) 
 Expressing 
disagreement  
Expressing 
disagreement with 
others or contents of 
others’ messages 
I completely agree about punishment 
issue but for rewards and praise I 
think they work in classrooms. // But, 
I still think that writing a story on pen 
and paper is not that boring. 
Appendix B. Fall and Spring Semester SNA Freeman’s Centrality Measures 
Fall Semester (36 Pre-Service Teachers, 4 Instructors) 
 Degree NrmDegree OutDegree NrmOutDegree InDegree NrmInDegree 
38 22 56.41 22 56.41 11 28.21 
40 14 35.90 14 35.90 6 15.39 
37 13 33.33 13 33.33 4 10.26 
18 7 17.95 1 2.56 7 17.95 
3 5 12.82 3 7.69 3 7.69 
4 5 12.82 3 7.69 4 10.26 
2 4 10.26 3 7.69 2 5.13 
10 4 10.26 3 7.69 3 7.69 
12 4 10.26 3 7.69 3 7.69 
28 4 10.26 0 5.13 4 10.26 
H. Müge Satar and Sumru Akcan 181 
 
11 3 7.69 1 2.56 2 5.13 
22 3 7.69 3 7.69 2 5.13 
33 3 7.69 0 0.00 3 7.69 
6 2 5.13 1 2.56 2 5.13 
29 2 5.13 0 0.00 2 5.13 
24 2 5.13 1 2.56 2 5.13 
1 2 5.13 0 0.00 2 5.13 
16 2 5.13 1 2.56 2 5.13 
30 2 5.13 1 2.56 1 2.56 
20 2 5.13 1 2.56 2 5.13 
26 2 5.13 0 0.00 2 5.13 
13 2 5.13 1 2.56 2 5.13 
9 1 2.56 0 0.00 1 2.56 
7 1 2.56 0 0.00 1 2.56 
25 1 2.56 1 2.56 1 2.56 
14 1 2.56 0 0.00 1 2.56 
27 1 2.56 1 2.56 0 0.00 
32 1 2.56 0 0.00 1 2.56 
34 1 2.56 0 0.00 1 2.56 
8 1 2.56 0 0.00 1 2.56 
17 1 2.56 0 0.00 1 2.56 
21 0 0.00 0 0.00 0 0.00 
15 0 0.00 0 0.00 0 0.00 
19 0 0.00 0 0.00 0 0.00 
31 0 0.00 0 0.00 0 0.00 
36 0 0.00 0 0.00 0 0.00 
5 0 0.00 0 0.00 0 0.00 
23 0 0.00 0 0.00 0 0.00 
39 0 0.00 0 0.00 0 0.00 
35 0 0.00 0 0.00 0 0.00 
Fall Semester Descriptive Statistics 
 M SD Sum Var. SSQ MCSSQ Euc Norm Min Max N 
Degree 2.95 4.29 118.00 18.40 1,084.00 735.90 32.92 0.00 22.00 40 
NrmDegree 7.56 11.00 302.56 120.96 7,126.89 4,838.26 84.42 0.00 56.41 40 
OutDegree 1.98 4.36 79.00 19.02 917.00 760.98 30.28 0.00 22.00 40 
NrmOutDegree 5.06 11.18 202.56 125.08 6,028.93 5,003.12 77.65 0.00 56.41 40 
InDegree 1.98 2.16 79.00 4.67 343.00 186.98 18.52 0.00 11.00 40 
NrmInDegree 5.06 5.54 202.56 30.73 2,255.10 1,229.29 47.49 0.00 28.21 40 
182 Language Learning & Technology 
 
Spring Semester (19 Pre-Service Teachers, 2 Instructors) 
 Degree NrmDegree OutDegree NrmOutDegree InDegree NrmInDegree 
37 16 80.00 15 75.00 9 45.00 
39 9 45.00 9 45.00 4 20.00 
10 9 45.00 7 35.00 7 35.00 
18 8 40.00 5 25.00 6 30.00 
11 6 30.00 2 10.00 4 10.00 
13 6 30.00 2 10.00 5 25.00 
12 5 25.00 2 10.00 5 25.00 
16 5 25.00 3 15.00 2 10.00 
15 4 20.00 2 10.00 3 15.00 
20 4 20.00 0 0.00 4 20.00 
3 3 15.00 3 15.00 2 10.00 
6 3 15.00 2 10.00 2 10.00 
7 3 15.00 1 5.00 2 10.00 
8 3 15.00 2 10.00 1 5.00 
17 3 15.00 1 5.00 2 10.00 
22 2 10.00 2 10.00 1 5.00 
9 1 5.00 1 5.00 1 5.00 
14 1 5.00 1 5.00 0 0.00 
19 1 5.00 1 5.00 1 5.00 
5 1 5.00 1 5.00 0 0.00 
1 1 5.00 0 0.00 1 5.00 
Spring Semester Descriptive Statistics 
 M SD Sum Var. SSQ MCSSQ Euc Norm Min Max N 
Degree 4.48 3.58 94.00 12.82 690.00 269.24 26.27 1.00 16.00 21 
NrmDegree 22.38 17.90 470.00 320.52 17,250.00 6,730.95 131.34 5.00 80.00 21 
OutDegree 2.95 3.44 62.00 11.86 432.00 248.95 20.79 0.00 15.00 21 
NrmOutDegree 14.76 17.22 310.00 296.37 10,800.00 6,223.81 103.92 0.00 75.00 21 
InDegree 2.95 2.34 62.00 5.47 298.00 114.95 17.26 0.00 9.00 21 
NrmInDegree 14.76 11.70 310.00 136.85 7,450.00 2,873.81 86.31 0.00 45.00 21 
  
H. Müge Satar and Sumru Akcan 183 
 
Appendix C. Sample Posts Exemplifying Interaction Patterns in Extracts 1 and 2 
Sample 1 (From Extract 1) 
Line Post Interaction Pattern 
16 S28, Nov 21: <link to glogster poster> Pre-service teacher’s (S28) 
reply to initial instructor post 
17 T1, Nov 26: Hi <name of S28>, Looks like you have bit of 
everything :) I got a warning message that said it could not load 
everything, but I could see the rabbit, elephant, dolphin and 
magpie. Which one are you the most? <name of T1> 
Feedback from instructor (T1) 
18 S28, Nov 27: Well there is also a mouse there:) My participation 
pattern is complicated. I mean I visit most days like elephant. But 
I don't contribute much especially if others haven't made any 
comment :) From this aspect, my pattren is like a combination of 
the elephant and the mouse as I said before :) But of course I 
don't participate like a magpie :) 
Pre-service teacher’s (S28) 
response to instructor 
feedback 
Sample 2 (From Extract 2) 
Line Post Interaction Pattern 
30 S15, April 1: One of my cooperating teachers uses the first 5 
minutes of the lesson to grasp the full attention of the students. 
They have a "first 5 minutes rule". At this period, all the students 
put their hands on the desks and look into the teacher. During 
this time teacher gives instructions and explains the core of the 
topic. After, students are more free to talk. When they're really 
out of control, she shouts to control them. 
Pre-service teacher’s (S15) 
reply to initial post 
31 S3, April 2: In my school, they do something similar. At the 
beginning of each school day, the teacher and the students sit 
down on the floor in a circle, and they talk about what they are 
going to do that day, and what the teacher expects them to do and 
how she expects them to behave. Only after this discussion, they 
go back to their seats and their class starts. This way, they let the 
students know what they will be doing that day so that they can 
pay attention to them as they go. 
Feedback from a pre-service 
teacher (S3) to another pre-
service teacher (S15) 
32 T3, April 3: So they (the teacher and students) set up their 
expectations for the daily routine and follow them to make things 
go smoothly. I think setting up expectations/rules can be one of 
the thumbs-up principles for creating a learning community. 
Feedback from instructor (T3) 
to pre-service teacher (S3) 
33 S3, April 4: Yes, when students know what they are expected to 
do, they often maintain their interest and they can see that if they 
follow the class activities, they will fulfill the expectation, which 
in turn will hopefully lead to an effective learning. 
Pre-service teacher’s (S3) 
response to instructor 
feedback 
34 S10, April 12: This is a very good idea, too. The beginning of the 
lesson is very clear and the students know what they are supposed 
to to [sic] at that time. They are aware of their responsibilities. I 
like 'clarity' :) 
Feedback from a pre-service 
teacher (S10) to another pre-
service teacher (S15) 
184 Language Learning & Technology 
 
Appendix D. Social Presence Results for Participants Who Contributed to 
Discussions in Both Fall and Spring Semesters 
 Fall Spring 
Total No. of Words 14,670 11,891 
Total SP  691 684 
Total SP Density 47.10 57.52 
Expressing Emotions 13 7 
Use of Humor 18 10 
Self-Disclosure 167 59 
Emoticons, Punctuation, etc. 97 103 
Expressing Value 92 41 
Affective Total 387 220 
Affective Density 26.38 18.50 
Vocatives 29 65 
Inclusive Pronouns 126 100 
Phatics, Salutations 52 34 
Intersubjectivity 7 13 
Cohesive Total 214 212 
Cohesive Density 14.59 17.83 
Referring to Other’s Messages 34 107 
Asking Questions 18 33 
Complimenting, Appreciation 31 69 
Expressing Agreement 7 35 
Expressing Disagreement  0 8 
Interactive Total 90 252 
Interactive Density 6.13 21.19 
About the Authors 
H. Müge Satar, PhD is a lecturer in Applied Linguistics and TESOL at Newcastle University, where she 
teaches graduate courses and supervises MA and PhD students. Her research interests include CMC, 
language teaching and teacher training, TBLT, and social presence in online learner interactions. 
E-mail: muge.satar@ncl.ac.uk 
Sumru Akcan is an associate professor in the Department of Foreign Language Education at Boğaziçi 
University, Istanbul, Turkey. She teaches undergraduate and graduate courses in teacher education and 
second and foreign language teaching methodology. Her research focuses on pre-service language 
education and second and foreign language teaching pedagogy.  
E-mail: sumru.akcan@boun.edu.tr 
